<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Sorting my socks with deep learning — Part 1</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://saubury.github.io/index.xml">
		<link rel="canonical" href="https://saubury.github.io/202002_sortingmysockswithdeeplearningpart1/202002_sortingmysockswithdeeplearningpart1/">
		
		<link rel="shortcut icon" type="image/png" href="https://saubury.github.io/apple-touch-icon-precomposed.png">
		
		
		<meta name="generator" content="Hugo 0.68.3" />

		
		<meta property="og:title" content="Sorting my socks with deep learning — Part 1" />
		<meta property="og:type" content="article" />
		<meta property="og:image" content="https://saubury.github.io/202002_SortingMySocksWithDeepLearningPart1/00000000.png" />
		<meta property="og:description" content="" />
		<meta property="og:url" content="https://saubury.github.io/202002_sortingmysockswithdeeplearningpart1/202002_sortingmysockswithdeeplearningpart1/" />
		<meta property="og:site_name" content="Sorting my socks with deep learning — Part 1" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://saubury.github.io/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://saubury.github.io/css/story.css" />
		<link rel="stylesheet" href="https://saubury.github.io/css/descartes.css" />
		
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		

		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://saubury.github.io/js/story.js"></script>

	</head>
	<body class="ma0 bg-white section-202002_SortingMySocksWithDeepLearningPart1 page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://saubury.github.io/202002_SortingMySocksWithDeepLearningPart1/00000000.png'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://saubury.github.io/" title="Home">
						<img src="https://saubury.github.io/img/logo.jpg" class="w2 h2 br-100" alt="Simon Aubury Site" />
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="https://twitter.com/simonaubury/"><i class="fab fa-twitter-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/saubury/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/simonaubury/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://saubury.github.io/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://saubury.github.io/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">Sorting my socks with deep learning — Part 1</h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2020-02-10T12:00:00&#43;11:00">Feb 10, 2020</time>
								<span class="display-print">by </span>
								
								<span class="display-print">at https://saubury.github.io/202002_sortingmysockswithdeeplearningpart1/202002_sortingmysockswithdeeplearningpart1/</span>
							
						
					</h2>
				</div>

				
				
				
				

			</div>
		</header>
		
		<main role="main">
		
<article class="center bg-white br-3 pv1 ph4 lh-copy f5 nested-links mw7">
	<h1 id="sorting-my-socks-with-deep-learningpart-1">Sorting my socks with deep learning — Part 1</h1>
<p>Can I arrange my sock drawer using machine learning? An adventure in pairing my socks using transfer learning to build a custom sock image classification model. A small project to train and deploy an object recognition model on AWS DeepLens hardware for identifying my socks.</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000000.png" alt="Sock classification madness to neatly paired"><em>Sock classification madness to neatly paired</em></p>
<h2 id="the-idea">The Idea</h2>
<p>I have a crazy idea that I can pair my mismatched socks with machine learning. That is, building an image recognition model on the socks I have sitting in my sock drawer. With sufficient training data I should be able to tech a machine to classify a sock, and determine when I have a pair of identical socks. No more mismatched socks!</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000001.gif" alt="A deep learning model saving me from household chores"><em>A deep learning model saving me from household chores</em></p>
<p>This project is also a good chance to test (or justify?) my new <a href="https://aws.amazon.com/deeplens/">AWS Deeplens</a> — a “deep-learning enabled video camera”. A Deeplens is a $300 hardware device with a 4MP video camera, Intel Atom Processor, 8GB RAM and runs Ubuntu. Plenty of hardware to help sort my socks.</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000002.png" alt="A Deeplens in sock-classification action"><em>A Deeplens in sock-classification action</em></p>
<p>This project and all the files are available <a href="https://github.com/saubury/socksort/">here</a>.</p>
<h2 id="image-capture">Image Capture</h2>
<p>Any good machine vision project starts with training data. In my case, I need to prepare a set of training images to teach a model to recognise similar classification of socks. More specifically, I need to take a lot of photos of socks, and classify the images so I can run a supervised learning image classification. Much like you can teach a child to recognise something with a few examples, so too a ML model. With enough images of socks (tagged with their known sock type) a model can be created to classify socks into their correct type.</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000003.png" alt="One sock at a time gets a photo"><em>One sock at a time gets a photo</em></p>
<p>Taking photos of socks is just as exciting as it sounds. Luckily I found around 30 photos of each sock were sufficient to accurately identify each sock variety— so it didn’t take too long to capture images.</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000004.png" alt="Each sock has around 30 different photos"><em>Each sock has around 30 different photos</em></p>
<p>To save time, I made sure each sock filled the camera frame. This means I didn’t need to add bounding boxes to the images as the entire image frame was filled with a single sock image.</p>
<h2 id="image-preparation">Image Preparation</h2>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000005.png" alt="The exciting task of image preparation"><em>The exciting task of image preparation</em></p>
<p>While you can train a model on a folder full of images, it’s preferable to optimise the format of the images so that it’s less IO intensive during model training. I’ll prepare my dataset of sock images in <a href="https://gluon-cv.mxnet.io/build/examples_datasets/recordio.html">ImageRecord</a> format.</p>
<p>To prepare my sock dataset in ImageRecord format I first need to generate a .lst file, a text file describing images classification and filename. I can use the Apache MXNet* im2rec.py* python file to help with these steps</p>
<pre><code>python3 -m venv myenv
source myenv/bin/activate
pip install mxnet opencv-python
curl --output im2rec.py [https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py](https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py)
</code></pre>
<p>I chose to split the photos into 80% training images and 20% for model validation. My data had 8 classification (ie., 8 different sock varieties) each with around 30 images. To randomly allocate the images into training and validation sets I run</p>
<pre><code>python im2rec.py --list  --train-ratio 0.8   --recursive ./sock-images_rec sock-images/
</code></pre>
<p>After the execution, you find files sock-images_rec_train.lst and sock-images_rec_val.lst generated. These text files describe the images and classification for training and validation sets</p>
<pre><code>wc -l *.lst
544 sock-images_rec_train.lst
137 sock-images_rec_val.lst
681 total
</code></pre>
<p>Now I want to actually crunch my sock images into ImageRecord, reducing the size of each image down to a 512x512 pixel image.</p>
<pre><code>python im2rec.py   --resize 512   --center-crop   --num-thread 4 ./sock-images_rec ./sock-images/
</code></pre>
<p>It gives you four more files: (sock-images_rec_train.idx, sock-images_rec_train.rec, sock-images_rec_val.idx, sock-images_rec_val.rec). Now, I’ve got ImageRecord files for both the training and validation. I’ll copy these up to an s3 so I can run the training in d’ cloud.</p>
<pre><code>aws s3 cp . s3://deeplens-sagemaker-socksort --exclude &quot;*&quot; --include &quot;*.idx&quot;  --include &quot;*.rec&quot;  --include &quot;*.lst&quot; --recursive
</code></pre>
<h2 id="model-training">Model Training</h2>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000006.png" alt=""></p>
<p>Now we want to train an image classification model that can classify sock images. We will use <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> to leverage the knowledge gained from similar classification tasks to bootstrap my sock classification model. I can run this model training in a hosted Jupyter Notebook (AWS Sagemaker) in Python.</p>
<p>The basic steps being</p>
<ul>
<li>
<p>Import the sock dataset as a <em>recordio</em> format.</p>
</li>
<li>
<p>Build an image classification model — see <a href="https://github.com/saubury/socksort/blob/master/20-model-training/sock-classification.ipynb">*sock-classification.ipynb</a>*. All going well after running this notebook you should have a model file called${S3_BUCKET}/ic-transfer-learning/output/image-classification-${DATE}/output/model.tar.gz</p>
</li>
<li>
<p>Deploy a temporary classifier to test the inference function (that is, spin up your model with a temporary end point)</p>
</li>
<li>
<p>Test a few demonstration images can be correctly classified</p>
</li>
<li>
<p>Convince myself this is time well spent</p>
</li>
</ul>
<p>A more extensive explanation <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-transfer-learning-highlevel.ipynb">here</a></p>
<h2 id="deeplens-mxnet-model--lambda-function">Deeplens MXNet Model &amp; Lambda Function</h2>
<p>So far we’ve managed to train a model, and test it with a cloud hosted interactive Jupyter Notebook. Now I want to have this model running on my local hardware against images captured and processed on the Deeplens camera hardware. I’ll need to deploy the trained model locally and write a local lambda python function.</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000007.png" alt=""></p>
<p>To create this local run-time project we need to build, publish and deploy a Sock Sort AWS DeepLens Inference Lambda Function. This is essentially a zip file containing my Python code (which pulls images from the camera and calls the classification model) along with support Python libraries (such as MQTT broker libraries) along with basic configuration settings.</p>
<p>Steps to build sock_deeplens_inference_function.zip are detailed <a href="https://github.com/saubury/socksort/blob/master/README.md#deeplens-lambda-function">here</a>. So too are the steps required to load our previously created object classification model into my Deeplens hardware.</p>
<h2 id="publish-sock-sort-aws-deeplens-inference-lambda-function">Publish Sock Sort AWS DeepLens Inference Lambda Function</h2>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000008.png" alt="DeepLens with model and local Lambda sending predictions to browser and MQTT"><em>DeepLens with model and local Lambda sending predictions to browser and MQTT</em></p>
<p>Now we have a trained model and Python Inference Lambda Function deployed locally to my Deeplens hardware. Powering up the device should run all images captured by the camera through the classification model. If a likely match is found, the details are overlaid on the image. I can review a live camera feed within as web-browser, and a text overlay displays the sock classification. I’m also pushing the classification prediction into an MQTT topic (more about that in part-2).</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000009.png" alt="Socks — identified with model"><em>Socks — identified with model</em></p>
<h2 id="to-be-continued-">To be continued …</h2>
<p>Thanks for reading this far. We’ve got a model running locally that can identify single socks … but we’re only half way there! Our next part of the process needs to be able to identify our matching socks.</p>
<p>Have a look at <a href="https://medium.com/@simon.aubury/kafka-stream-processing-sorting-socks-with-ksqldb-e4174ae5e703">p</a>art 2 — <a href="https://medium.com/@simon.aubury/kafka-stream-processing-sorting-socks-with-ksqldb-e4174ae5e703">Kafka stream processing: sorting socks with ksqlDB</a>.</p>
<p><img src="/202002_SortingMySocksWithDeepLearningPart1/00000010.png" alt="Socks — paired!"><em>Socks — paired!</em></p>
<h3 id="project-files">Project Files</h3>
<p>Project files with code used available at <a href="https://github.com/saubury/socksort/">https://github.com/saubury/socksort/</a></p>

</article>

		</main>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					
				
			
		
		</div>
		
	</div>

		
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://saubury.github.io/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2021 
			</p>
		</footer>
		
	
	
	</body>
</html>
